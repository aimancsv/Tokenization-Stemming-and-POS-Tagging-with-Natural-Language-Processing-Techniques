{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4CfsJMWdXqZ"
      },
      "source": [
        "TXTA Assignment\n",
        "Q1. Form tokenization and Filter stop words & punctuation (12 marks)\n",
        "1.\tDemonstrate word tokenisation using the split function, Regular Expression and NLTK packages separately and report the output. (4 marks) (code)\n",
        "2.\tJustify the most suitable tokenisation operation for text analytics. Support your answer using obtained outputs. (2 marks) (doc)\n",
        "3.\tDemonstrate stop words and punctuations removal and report the output suitably along with the stop words found in the given text corpus. (4 marks) (code)\n",
        "4.\tExplain the importance of filtering the stop words and punctuations in text analytics. (2 marks) (doc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eSn39IOHP2xY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuRnMPdJdXqa"
      },
      "source": [
        "Load File and Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "yRNiAqZ2dlBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTU4SgL8dXqa"
      },
      "outputs": [],
      "source": [
        "file = open('/content/drive/MyDrive/Colab Notebooks/TXSA/Assignment Data/Data_1.txt')\n",
        "text = file.read()\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download(\"popular\")\n"
      ],
      "metadata": {
        "id": "GM0mBhWLST3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ep2IM5hfdXqb"
      },
      "source": [
        "# Part 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2ilBDPddXqb",
        "outputId": "69981475-16f4-4d93-9d07-24e93613bd03"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokenization using Split: \n",
            "['Textual', 'information', 'in', 'the', 'world', 'can', 'be', 'broadly', 'categorized', 'into', 'two', 'main', 'types:', 'facts', 'and', 'opinions.', 'Facts', 'are', 'objective', 'expressions', 'about', 'entities,', 'events,', 'and', 'their', 'properties.', 'Opinions', 'are', 'usually', 'subjective', 'expressions', 'that', 'describe', \"people's\", 'sentiments,', 'appraisals,', 'or', 'feelings', 'toward', 'entities,', 'events,', 'and', 'their', 'properties.'] \n",
            "\n",
            "Word Tokenization using RegEx: \n",
            "['Textual', 'information', 'in', 'the', 'world', 'can', 'be', 'broadly', 'categorized', 'into', 'two', 'main', 'types:', 'facts', 'and', 'opinions.', 'Facts', 'are', 'objective', 'expressions', 'about', 'entities,', 'events,', 'and', 'their', 'properties.', 'Opinions', 'are', 'usually', 'subjective', 'expressions', 'that', 'describe', \"people's\", 'sentiments,', 'appraisals,', 'or', 'feelings', 'toward', 'entities,', 'events,', 'and', 'their', 'properties.\\n'] \n",
            "\n",
            "Word Tokenization using NLTK: \n",
            "['Textual', 'information', 'in', 'the', 'world', 'can', 'be', 'broadly', 'categorized', 'into', 'two', 'main', 'types', ':', 'facts', 'and', 'opinions', '.', 'Facts', 'are', 'objective', 'expressions', 'about', 'entities', ',', 'events', ',', 'and', 'their', 'properties', '.', 'Opinions', 'are', 'usually', 'subjective', 'expressions', 'that', 'describe', 'people', \"'s\", 'sentiments', ',', 'appraisals', ',', 'or', 'feelings', 'toward', 'entities', ',', 'events', ',', 'and', 'their', 'properties', '.']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "#1. Demonstrate word tokenisation using the split function, Regular Expression and NLTK packages separately and report the output.\n",
        "\n",
        "# split function\n",
        "a = text.split()\n",
        "print(\"Word Tokenization using Split: \")\n",
        "print(a, '\\n')\n",
        "\n",
        "# Regular Expression\n",
        "regex = re.split(\" \", text)\n",
        "print(\"Word Tokenization using RegEx: \")\n",
        "print(regex, '\\n')\n",
        "\n",
        "# NLTK\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Word Tokenization using NLTK: \")\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ps1tvhKdXqc"
      },
      "source": [
        "Correct punctuation may not be separated if the \".split\" function is used to separate modifications made to RE.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjWxPFYVdXqc",
        "outputId": "0b1cc719-110d-4aeb-9ec2-dcb9c536571a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split function output:\n",
            "['Textual', 'information', 'in', 'the', 'world', 'can', 'be', 'broadly', 'categorized', 'into', 'two', 'main', 'types:', 'facts', 'and', 'opinions.', 'Facts', 'are', 'objective', 'expressions', 'about', 'entities,', 'events,', 'and', 'their', 'properties.', 'Opinions', 'are', 'usually', 'subjective', 'expressions', 'that', 'describe', \"people's\", 'sentiments,', 'appraisals,', 'or', 'feelings', 'toward', 'entities,', 'events,', 'and', 'their', 'properties.']\n",
            "\n",
            "Regular Expression output:\n",
            "['Textual', 'information', 'in', 'the', 'world', 'can', 'be', 'broadly', 'categorized', 'into', 'two', 'main', 'types', 'facts', 'and', 'opinions', 'Facts', 'are', 'objective', 'expressions', 'about', 'entities', 'events', 'and', 'their', 'properties', 'Opinions', 'are', 'usually', 'subjective', 'expressions', 'that', 'describe', 'people', 's', 'sentiments', 'appraisals', 'or', 'feelings', 'toward', 'entities', 'events', 'and', 'their', 'properties']\n",
            "\n",
            "NLTK output:\n",
            "['Textual', 'information', 'in', 'the', 'world', 'can', 'be', 'broadly', 'categorized', 'into', 'two', 'main', 'types', ':', 'facts', 'and', 'opinions', '.', 'Facts', 'are', 'objective', 'expressions', 'about', 'entities', ',', 'events', ',', 'and', 'their', 'properties', '.', 'Opinions', 'are', 'usually', 'subjective', 'expressions', 'that', 'describe', 'people', \"'s\", 'sentiments', ',', 'appraisals', ',', 'or', 'feelings', 'toward', 'entities', ',', 'events', ',', 'and', 'their', 'properties', '.']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Read the data from the file\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/TXSA/Assignment Data/Data_1.txt\", \"r\") as file:\n",
        "    data = file.read()\n",
        "\n",
        "# Using split function for tokenization\n",
        "split_tokens = data.split()\n",
        "print(\"Split function output:\")\n",
        "print(split_tokens)\n",
        "\n",
        "# Using Regular Expression for tokenization\n",
        "# Utilise <b to identify the beginning and ending of the word, followed by <w+ to verify one or more letters or numbers..\n",
        "regex_tokens = re.findall(r'\\b\\w+\\b', data)\n",
        "print(\"\\nRegular Expression output:\")\n",
        "print(regex_tokens)\n",
        "\n",
        "# Using NLTK for tokenization\n",
        "nltk_tokens = word_tokenize(data)\n",
        "print(\"\\nNLTK output:\")\n",
        "print(nltk_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcJM-YwndXqc",
        "outputId": "d0a16932-4fd0-4af7-8015-24a880c05cb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Join sentence:  textual information world broadly categorized two main types facts opinions facts objective expressions entities events properties opinions usually subjective expressions describe people sentiments appraisals feelings toward entities events properties\n"
          ]
        }
      ],
      "source": [
        "#3. Demonstrate stop words and punctuations removal and report the output suitably along with the stop words found in the given text corpus.\n",
        "\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def remove_stopwords_punctuation(txt):\n",
        "\n",
        "    # derive stop words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    # tokenize the text\n",
        "    ab = word_tokenize(txt)\n",
        "\n",
        "    # removing stop words and punctuations\n",
        "    filtered_words = [word.lower() for word in ab if word.lower()\n",
        "    not in stop_words and word.isalpha()]\n",
        "\n",
        "\n",
        "    # Join back into a sentence\n",
        "    fresult = ' '.join(filtered_words)\n",
        "\n",
        "    return fresult\n",
        "\n",
        "fresultnew = remove_stopwords_punctuation(text)\n",
        "print(\"Join sentence: \", fresultnew)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgSHHNtndXqc"
      },
      "source": [
        ":Rather than entering the stop words by hand, we can utilise the library's classification system and print them for comparative purposes.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c06IeKzudXqc",
        "outputId": "097d3dee-3f3b-463e-d1ea-e48e4e75c2ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Tokens after Stop Words and Punctuation Removal:\n",
            "['Textual', 'information', 'world', 'broadly', 'categorized', 'two', 'main', 'types', 'facts', 'opinions', 'Facts', 'objective', 'expressions', 'entities', 'events', 'properties', 'Opinions', 'usually', 'subjective', 'expressions', 'describe', 'people', \"'s\", 'sentiments', 'appraisals', 'feelings', 'toward', 'entities', 'events', 'properties']\n",
            "\n",
            "Stop Words Found in the Text:\n",
            "{'ll', 'had', 'because', 'here', 'is', \"weren't\", 'as', \"hadn't\", 'wasn', 'such', \"needn't\", 'all', \"shan't\", 'only', 'an', 'mustn', 'me', 'a', \"should've\", \"wasn't\", 'i', 'during', 're', 'if', \"she's\", \"you'd\", 'about', 'over', 'hasn', 't', 'wouldn', 'to', 'nor', 'no', 'then', 'out', 'don', \"don't\", 'yourself', 'until', 'what', 'again', 'under', 'didn', 'she', 'where', 'same', 'and', 'hadn', 'other', 'his', \"aren't\", \"hasn't\", 'you', 'when', 'doesn', 'so', 'at', 'before', 'should', 'needn', 'there', 'having', 'but', 'haven', 'more', 'their', 'it', 'himself', 'been', 'between', 'couldn', \"you'll\", 'shouldn', 'or', 'in', 'shan', 'above', 'that', 'from', \"mightn't\", \"wouldn't\", 'this', 'ourselves', 'own', \"you're\", 'does', 'some', 'both', 'below', 'who', 'how', 'have', 'through', 'while', 'theirs', 'he', 'am', \"mustn't\", \"haven't\", 'won', 'has', 'weren', 'yours', 'on', 'which', 'o', 's', 'them', 'few', \"couldn't\", 'once', 'ours', 'your', 'further', 'ma', 'they', 'do', 'against', 'mightn', 'ain', 'by', 'did', 'into', 'our', 'we', 'these', 'each', 'themselves', \"didn't\", 'than', 'myself', \"won't\", 'not', 've', 'of', 'hers', 'being', 'can', \"you've\", 'was', 'are', 'aren', 'were', 'why', 'whom', 'down', 'isn', 'be', 'too', 'yourselves', \"isn't\", 'its', 'for', \"shouldn't\", 'very', 'up', 'doing', 'most', 'just', 'y', 'the', 'now', 'itself', 'any', 'my', \"it's\", 'after', 'off', \"doesn't\", 'herself', 'him', 'with', 'will', \"that'll\", 'those', 'd', 'her', 'm'}\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(data)\n",
        "\n",
        "# Remove punctuation\n",
        "tokens_no_punct = [word for word in tokens if word not in string.punctuation]\n",
        "\n",
        "# Remove stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens_no_punct if word.lower() not in\n",
        "                   stop_words]\n",
        "\n",
        "# Print the filtered tokens and stop words\n",
        "print(\"Filtered Tokens after Stop Words and Punctuation Removal:\")\n",
        "print(filtered_tokens)\n",
        "\n",
        "print(\"\\nStop Words Found in the Text:\")\n",
        "print(stop_words)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}